{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_FOLDER = \"./train/images/\"\n",
    "TRAIN_LABEL_FOLDER = \"./train/annotations/\"\n",
    "\n",
    "TEST_IMAGE_FOLDER = \"./test/images/\"\n",
    "TEST_LABEL_FOLDER = \"./test/annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "label_paths = []\n",
    "\n",
    "for file in os.listdir(TRAIN_IMAGE_FOLDER):\n",
    "    image_paths.append(os.path.join(TRAIN_IMAGE_FOLDER, file))\n",
    "\n",
    "for file in os.listdir(TRAIN_LABEL_FOLDER):\n",
    "    label_paths.append(os.path.join(TRAIN_LABEL_FOLDER, file))\n",
    "\n",
    "print(\"Number of images: \", len(image_paths))\n",
    "print(\"Number of labels: \", len(label_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "ground_truths = []\n",
    "keypoints_ground_truths = {}\n",
    "text_detection_ground_truths = {}\n",
    "x_labels_text_detection_ground_truths = {}\n",
    "y_labels_text_detection_ground_truths = {}\n",
    "\n",
    "for path in tqdm(label_paths):\n",
    "    # if \"17000b60f53e\" not in path:\n",
    "    #     continue\n",
    "    with open(path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    image_name = os.path.basename(path)[:-5] + \".jpg\"\n",
    "\n",
    "    for item in data[\"data-series\"]:\n",
    "        for k in item:\n",
    "            if isinstance(item[k], float):\n",
    "                item[k] = round(item[k], 4) \n",
    "\n",
    "    x_axis_ids = [item[\"id\"] for item in data[\"axes\"][\"x-axis\"][\"ticks\"]]\n",
    "    y_axis_ids = [item[\"id\"] for item in data[\"axes\"][\"y-axis\"][\"ticks\"]]\n",
    "\n",
    "    x_labels = [item for item in data[\"text\"] if item[\"id\"] in x_axis_ids]\n",
    "    x_labels.sort(key=lambda x: x[\"polygon\"][\"x0\"])\n",
    "    x_labels = [item[\"text\"].strip() for item in x_labels]\n",
    "    x_labels_polygons = [item[\"polygon\"] for item in data[\"text\"] if item[\"id\"] in x_axis_ids]\n",
    "    x_labels_rects = []\n",
    "    for polygon in x_labels_polygons:\n",
    "        xs = [polygon[\"x0\"], polygon[\"x1\"], polygon[\"x2\"], polygon[\"x3\"]]\n",
    "        ys = [polygon[\"y0\"], polygon[\"y1\"], polygon[\"y2\"], polygon[\"y3\"]]\n",
    "\n",
    "        x_labels_rects.append([min(xs), min(ys), max(xs), max(ys)])\n",
    "\n",
    "    y_labels = [item for item in data[\"text\"] if item[\"id\"] in y_axis_ids]\n",
    "    y_labels.sort(key=lambda x: x[\"polygon\"][\"y0\"])\n",
    "    y_labels = [item[\"text\"].strip() for item in y_labels]\n",
    "    y_labels_polygons = [item[\"polygon\"] for item in data[\"text\"] if item[\"id\"] in y_axis_ids]\n",
    "    y_labels_rects = []\n",
    "    for polygon in y_labels_polygons:\n",
    "        xs = [polygon[\"x0\"], polygon[\"x1\"], polygon[\"x2\"], polygon[\"x3\"]]\n",
    "        ys = [polygon[\"y0\"], polygon[\"y1\"], polygon[\"y2\"], polygon[\"y3\"]]\n",
    "\n",
    "        y_labels_rects.append([min(xs), min(ys), max(xs), max(ys)])\n",
    "\n",
    "    gt = {\n",
    "        \"file_name\": os.path.join(\"images\", image_name),\n",
    "        \"ground_truth\": {\n",
    "            \"gt_parse\": {\n",
    "                \"class\": data[\"chart-type\"],\n",
    "                \"value\": data[\"data-series\"],\n",
    "                \"x_type\": data[\"axes\"][\"x-axis\"][\"values-type\"],\n",
    "                \"y_type\": data[\"axes\"][\"y-axis\"][\"values-type\"],\n",
    "                \"x_labels\": x_labels,\n",
    "                \"y_labels\": y_labels,\n",
    "                \"x_labels_polygons\": x_labels_polygons,\n",
    "                \"y_labels_polygons\": y_labels_polygons,\n",
    "                \"x_labels_rects\": x_labels_rects,\n",
    "                \"y_labels_rects\": y_labels_rects,\n",
    "            }\n",
    "        },\n",
    "        \"source\": data[\"source\"],\n",
    "    }\n",
    "    ground_truths.append(gt)\n",
    "\n",
    "    # TODO: for multi stages pipeline, we need to save the points of each axis and the points of each data series\n",
    "    # we need to calculate pixel value of each point in data-series based on axis points\n",
    "    x_data = []\n",
    "    for tick in data[\"axes\"][\"x-axis\"][\"ticks\"]:\n",
    "        x_data.append(\n",
    "            {\n",
    "                \"id\": tick[\"id\"],\n",
    "                \"x\": tick[\"tick_pt\"][\"x\"],\n",
    "                \"y\": tick[\"tick_pt\"][\"y\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    y_data = []\n",
    "    for tick in data[\"axes\"][\"y-axis\"][\"ticks\"]:\n",
    "        y_data.append(\n",
    "            {\n",
    "                \"id\": tick[\"id\"],\n",
    "                \"x\": tick[\"tick_pt\"][\"x\"],\n",
    "                \"y\": tick[\"tick_pt\"][\"y\"],\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # add label to x_data and y_data\n",
    "    x_id_to_text = {item[\"id\"]: item[\"text\"] for item in data[\"text\"] if item[\"id\"] in x_axis_ids}\n",
    "    y_id_to_text = {item[\"id\"]: item[\"text\"] for item in data[\"text\"] if item[\"id\"] in y_axis_ids}\n",
    "\n",
    "    for item in x_data:\n",
    "        item[\"text\"] = x_id_to_text[item[\"id\"]]\n",
    "    \n",
    "    for item in y_data:\n",
    "        item[\"text\"] = y_id_to_text[item[\"id\"]]\n",
    "\n",
    "    # x_data_dict = {item[\"text\"]: item for item in x_data}\n",
    "    # y_data_dict = {item[\"text\"]: item for item in y_data}\n",
    "    from collections import defaultdict\n",
    "    x_data_dict = defaultdict(list)\n",
    "    for item in x_data:\n",
    "        x_data_dict[item[\"text\"]].append(item)\n",
    "    \n",
    "    y_data_dict = defaultdict(list)\n",
    "    for item in y_data:\n",
    "        y_data_dict[item[\"text\"]].append(item)\n",
    "\n",
    "    # calculate pixel values\n",
    "    x_type = data[\"axes\"][\"x-axis\"][\"values-type\"]\n",
    "    y_type = data[\"axes\"][\"y-axis\"][\"values-type\"]\n",
    "\n",
    "    skip = False\n",
    "    for item in data[\"data-series\"]:\n",
    "        try:\n",
    "            if x_type == \"numerical\":\n",
    "                x_value = float(item[\"x\"])\n",
    "                x_data.sort(key=lambda x: abs(float(x[\"text\"].replace(\",\", \".\").replace(\"%\", \"\")) - x_value))\n",
    "                x1 = x_data[0]\n",
    "                x2 = x_data[1]\n",
    "                x1_value = float(x1[\"text\"].replace(\",\", \".\").replace(\"%\", \"\"))\n",
    "                x2_value = float(x2[\"text\"].replace(\",\", \".\").replace(\"%\", \"\"))\n",
    "\n",
    "                # print(\"x_value: \", x_value, \"x1_value: \", x1_value, \"x2_value: \", x2_value, \"x1: \", x1, \"x2: \", x2)\n",
    "                # TODO: check this, make it similar to pipeline.ipynb\n",
    "                if x_value > float(x1[\"text\"]):\n",
    "                    x_pixel = (x_value - x1_value) / (x2_value - x1_value) * (x2[\"x\"] - x1[\"x\"]) + x1[\"x\"]\n",
    "                else:\n",
    "                    x_pixel = x1[\"x\"] - (x1_value - x_value) / (x2_value - x1_value) * (x2[\"x\"] - x1[\"x\"])\n",
    "            else: # categorical\n",
    "                x_value = item[\"x\"]\n",
    "                x_pixel = x_data_dict[x_value].pop(0)[\"x\"]\n",
    "                # print(x_value)\n",
    "            \n",
    "            if y_type == \"numerical\":\n",
    "                y_value = float(item[\"y\"])\n",
    "                y_data.sort(key=lambda x: abs(float(x[\"text\"].replace(\",\", \".\").replace(\"%\", \"\")) - y_value))\n",
    "                y1 = y_data[0]\n",
    "                y2 = y_data[1]\n",
    "                y1_value = float(y1[\"text\"].replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "                y2_value = float(y2[\"text\"].replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "\n",
    "                # print(\"y_value =\", y_value, \"y1_value =\", y1_value, \"y2_value =\", y2_value, \"y1[y] =\", y1[\"y\"], \"y2[y] =\", y2[\"y\"])\n",
    "                # TODO: check this, make it similar to pipeline.ipynb\n",
    "                if y_value > y1_value:\n",
    "                    y_pixel = (y_value - y1_value) / (y2_value - y1_value) * (y2[\"y\"] - y1[\"y\"]) + y1[\"y\"]\n",
    "                else:\n",
    "                    y_pixel = y1[\"y\"] - (y1_value - y_value) / (y2_value - y1_value) * (y2[\"y\"] - y1[\"y\"])\n",
    "            else: # categorical\n",
    "                y_value = item[\"y\"]\n",
    "                y_pixel = y_data_dict[y_value].pop(0)[\"y\"]\n",
    "\n",
    "            item[\"x_pixel\"] = x_pixel\n",
    "            item[\"y_pixel\"] = y_pixel\n",
    "        except:\n",
    "            skip = True\n",
    "            break\n",
    "    # # DEBUG: plot data-series points to image\n",
    "    # image_path = os.path.join(TRAIN_IMAGE_FOLDER, image_name)\n",
    "    # image = cv2.imread(image_path)\n",
    "\n",
    "    # for item in data[\"data-series\"]:\n",
    "    #     cv2.circle(image, (int(item[\"x_pixel\"]), int(item[\"y_pixel\"])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # # cv2.imwrite(os.path.join(\"debug\", image_name), image)\n",
    "    # plt.imshow(image)\n",
    "    # plt.show()\n",
    "    # break\n",
    "\n",
    "\n",
    "    if not skip:\n",
    "        keypoints_ground_truths[image_name] = {\n",
    "            \"value\": data[\"data-series\"],\n",
    "            \"x\": data[\"axes\"][\"x-axis\"][\"ticks\"],\n",
    "            \"y\": data[\"axes\"][\"y-axis\"][\"ticks\"],\n",
    "            \"x_label\": x_labels_rects,\n",
    "            \"y_label\": y_labels_rects,\n",
    "        }\n",
    "\n",
    "    # prepare polygon ground truth\n",
    "    polygon_text = \"\"\n",
    "    for item in data[\"text\"]:\n",
    "        x0 = item[\"polygon\"][\"x0\"]\n",
    "        y0 = item[\"polygon\"][\"y0\"]\n",
    "        x1 = item[\"polygon\"][\"x1\"]\n",
    "        y1 = item[\"polygon\"][\"y1\"]\n",
    "        x2 = item[\"polygon\"][\"x2\"]\n",
    "        y2 = item[\"polygon\"][\"y2\"]\n",
    "        x3 = item[\"polygon\"][\"x3\"]\n",
    "        y3 = item[\"polygon\"][\"y3\"]\n",
    "        polygon_text += f\"{x0},{y0},{x1},{y1},{x2},{y2},{x3},{y3},dummy_text\\n\"\n",
    "\n",
    "    x_labels_poly_text = \"\"\n",
    "    for item in data[\"text\"]:\n",
    "        if item[\"id\"] in x_axis_ids:\n",
    "            x0 = item[\"polygon\"][\"x0\"]\n",
    "            y0 = item[\"polygon\"][\"y0\"]\n",
    "            x1 = item[\"polygon\"][\"x1\"]\n",
    "            y1 = item[\"polygon\"][\"y1\"]\n",
    "            x2 = item[\"polygon\"][\"x2\"]\n",
    "            y2 = item[\"polygon\"][\"y2\"]\n",
    "            x3 = item[\"polygon\"][\"x3\"]\n",
    "            y3 = item[\"polygon\"][\"y3\"]\n",
    "            x_labels_poly_text += f\"{x0},{y0},{x1},{y1},{x2},{y2},{x3},{y3},dummy_text\\n\"\n",
    "\n",
    "    y_labels_poly_text = \"\"\n",
    "    for item in data[\"text\"]:\n",
    "        if item[\"id\"] in y_axis_ids:\n",
    "            x0 = item[\"polygon\"][\"x0\"]\n",
    "            y0 = item[\"polygon\"][\"y0\"]\n",
    "            x1 = item[\"polygon\"][\"x1\"]\n",
    "            y1 = item[\"polygon\"][\"y1\"]\n",
    "            x2 = item[\"polygon\"][\"x2\"]\n",
    "            y2 = item[\"polygon\"][\"y2\"]\n",
    "            x3 = item[\"polygon\"][\"x3\"]\n",
    "            y3 = item[\"polygon\"][\"y3\"]\n",
    "            y_labels_poly_text += f\"{x0},{y0},{x1},{y1},{x2},{y2},{x3},{y3},dummy_text\\n\"\n",
    "\n",
    "\n",
    "    text_detection_ground_truths[image_name] = polygon_text\n",
    "    x_labels_text_detection_ground_truths[image_name] = x_labels_poly_text\n",
    "    y_labels_text_detection_ground_truths[image_name] = y_labels_poly_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize text_detection_ground_truths\n",
    "import numpy as np\n",
    "# shuffle\n",
    "x_labels_text_detection_ground_truths = {k: x_labels_text_detection_ground_truths[k] for k in np.random.permutation(list(x_labels_text_detection_ground_truths.keys()))}\n",
    "for image_name, polygon_text in x_labels_text_detection_ground_truths.items():\n",
    "    image_path = os.path.join(TRAIN_IMAGE_FOLDER, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    for line in polygon_text.splitlines():\n",
    "        points = line.split(\",dummy_text\")[0].split(\",\")\n",
    "        points = [int(x) for x in points]\n",
    "        points = np.array(points).reshape(-1, 2)\n",
    "        cv2.polylines(image, [points], True, (0, 255, 0), 2)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_item_from_source = {}\n",
    "for item in ground_truths:\n",
    "    if item[\"source\"] not in num_item_from_source:\n",
    "        num_item_from_source[item[\"source\"]] = 0\n",
    "    num_item_from_source[item[\"source\"]] += 1\n",
    "\n",
    "print(\"Number of items from each source: \", num_item_from_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item that has source = \"extracted\"\n",
    "source_image_paths = []\n",
    "for item in ground_truths:\n",
    "    if item[\"source\"] == \"extracted\":\n",
    "        source_image_paths.append(os.path.join(\"train\", item[\"file_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open(source_image_paths[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and validation image paths from train_list.txt and val_list.txt\n",
    "train_image_paths = []\n",
    "val_image_paths = []\n",
    "\n",
    "with open(\"train_list.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        train_image_paths.append(line.strip())\n",
    "\n",
    "with open(\"val_list.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        val_image_paths.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split ground truth based on this train and validation image paths\n",
    "train_ground_truths = []\n",
    "val_ground_truths = []\n",
    "\n",
    "for item in ground_truths:\n",
    "    image_path = os.path.basename(item[\"file_name\"])\n",
    "    if image_path in train_image_paths:\n",
    "        train_ground_truths.append(item)\n",
    "    elif image_path in val_image_paths:\n",
    "        val_ground_truths.append(item)\n",
    "    else:\n",
    "        raise Exception(\"Image path not found in train_list.txt or val_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split ground truths into train and validation, validation images is all images from source = \"extracted\"\n",
    "# train_ground_truths = []\n",
    "# val_ground_truths = []\n",
    "\n",
    "# for item in ground_truths:\n",
    "#     if item[\"source\"] != \"extracted\":\n",
    "#         train_ground_truths.append(item)\n",
    "\n",
    "# # split 50:50 extracted source images into train and validation\n",
    "# remain_ground_truths = []\n",
    "# for item in ground_truths:\n",
    "#     if item[\"source\"] == \"extracted\":\n",
    "#         remain_ground_truths.append(item)\n",
    "\n",
    "# # set seed for reproducibility\n",
    "# import random\n",
    "# random.seed(42)\n",
    "# random.shuffle(remain_ground_truths)\n",
    "\n",
    "# for i in range(len(remain_ground_truths)):\n",
    "#     if i < len(remain_ground_truths) // 2:\n",
    "#         train_ground_truths.append(remain_ground_truths[i])\n",
    "#     else:\n",
    "#         val_ground_truths.append(remain_ground_truths[i])\n",
    "\n",
    "# print(\"Number of train ground truths: \", len(train_ground_truths))\n",
    "# print(\"Number of val ground truths: \", len(val_ground_truths))\n",
    "\n",
    "# # print one example\n",
    "# print(val_ground_truths[0][\"ground_truth\"][\"gt_parse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names = set([os.path.basename(item[\"file_name\"]) for item in train_ground_truths])\n",
    "val_file_names = set([os.path.basename(item[\"file_name\"]) for item in val_ground_truths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split keypoints ground truths into train and validation\n",
    "train_keypoints_ground_truths = {}\n",
    "val_keypoints_ground_truths = {}\n",
    "\n",
    "for image_name, keypoints in keypoints_ground_truths.items():\n",
    "    if image_name in train_file_names:\n",
    "        train_keypoints_ground_truths[image_name] = keypoints\n",
    "    elif image_name in val_file_names:\n",
    "        val_keypoints_ground_truths[image_name] = keypoints\n",
    "\n",
    "# save to json \n",
    "with open(\"train_keypoints_ground_truths.json\", \"w\") as f:\n",
    "    json.dump(train_keypoints_ground_truths, f)\n",
    "\n",
    "with open(\"val_keypoints_ground_truths.json\", \"w\") as f:\n",
    "    json.dump(val_keypoints_ground_truths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all images to train/images and val/images\n",
    "import shutil\n",
    "\n",
    "# make dirs for validation\n",
    "os.makedirs(\"./validation/images/\", exist_ok=True)\n",
    "\n",
    "for gt in tqdm(val_ground_truths):\n",
    "    shutil.copy2(os.path.join(\"./train\", gt[\"file_name\"]), \"./validation/images/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to jsonl file\n",
    "import jsonlines\n",
    "\n",
    "with jsonlines.open(\"./train/metadata.jsonl\", mode=\"w\") as writer:\n",
    "    writer.write_all(train_ground_truths)\n",
    "\n",
    "with jsonlines.open(\"./validation/metadata.jsonl\", mode=\"w\") as writer:\n",
    "    writer.write_all(val_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary\n",
    "text_gts = y_labels_text_detection_ground_truths\n",
    "\n",
    "val_image_names = os.listdir(\"./validation/images\")\n",
    "train_image_names = set(text_gts.keys()) - set(val_image_names)\n",
    "\n",
    "print(\"train_image_names: \", len(train_image_names))\n",
    "print(\"val_image_names: \", len(val_image_names))\n",
    "\n",
    "TRAIN_TEXT_DETECTION_FOLDER = \"./y_labels_train_gts\"\n",
    "VAL_TEXT_DETECTION_FOLDER = \"./y_labels_val_gts\"\n",
    "\n",
    "# makedirs\n",
    "os.makedirs(TRAIN_TEXT_DETECTION_FOLDER, exist_ok=True)\n",
    "os.makedirs(VAL_TEXT_DETECTION_FOLDER, exist_ok=True)\n",
    "\n",
    "for image_name, text in tqdm(text_gts.items()):\n",
    "    if image_name in train_image_names:\n",
    "        with open(os.path.join(TRAIN_TEXT_DETECTION_FOLDER, image_name + \".txt\"), \"w\") as f:\n",
    "            f.write(text)\n",
    "    else:\n",
    "        with open(os.path.join(VAL_TEXT_DETECTION_FOLDER, image_name + \".txt\"), \"w\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "# train_list.txt and val_list.txt\n",
    "with open(\"./y_labels_train_list.txt\", \"w\") as f:\n",
    "    for image_name in train_image_names:\n",
    "        f.write(image_name + \"\\n\")\n",
    "\n",
    "with open(\"./y_labels_val_list.txt\", \"w\") as f:\n",
    "    for image_name in val_image_names:\n",
    "        f.write(image_name + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary\n",
    "text_gts = x_labels_text_detection_ground_truths\n",
    "\n",
    "val_image_names = os.listdir(\"./validation/images\")\n",
    "train_image_names = set(text_gts.keys()) - set(val_image_names)\n",
    "\n",
    "print(\"train_image_names: \", len(train_image_names))\n",
    "print(\"val_image_names: \", len(val_image_names))\n",
    "\n",
    "TRAIN_TEXT_DETECTION_FOLDER = \"./x_labels_train_gts\"\n",
    "VAL_TEXT_DETECTION_FOLDER = \"./x_labels_val_gts\"\n",
    "\n",
    "# makedirs\n",
    "os.makedirs(TRAIN_TEXT_DETECTION_FOLDER, exist_ok=True)\n",
    "os.makedirs(VAL_TEXT_DETECTION_FOLDER, exist_ok=True)\n",
    "\n",
    "for image_name, text in tqdm(text_gts.items()):\n",
    "    if image_name in train_image_names:\n",
    "        with open(os.path.join(TRAIN_TEXT_DETECTION_FOLDER, image_name + \".txt\"), \"w\") as f:\n",
    "            f.write(text)\n",
    "    else:\n",
    "        with open(os.path.join(VAL_TEXT_DETECTION_FOLDER, image_name + \".txt\"), \"w\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "# train_list.txt and val_list.txt\n",
    "with open(\"./x_labels_train_list.txt\", \"w\") as f:\n",
    "    for image_name in train_image_names:\n",
    "        f.write(image_name + \"\\n\")\n",
    "\n",
    "with open(\"./x_labels_val_list.txt\", \"w\") as f:\n",
    "    for image_name in val_image_names:\n",
    "        f.write(image_name + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "lens = []\n",
    "\n",
    "for gt in ground_truths:\n",
    "    l = len(gt[\"ground_truth\"][\"gt_parse\"][\"value\"])\n",
    "    lens.append(l)\n",
    "    if l > max_len:\n",
    "        max_len = l\n",
    "\n",
    "print(\"Max number of data-series: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw histogram of length of data-series\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lens, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Levenshtein as lev\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def sigmoid2(x):\n",
    "    return 2 - 2 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sqrt(np.mean(np.square(y_true - y_pred)))\n",
    "\n",
    "\n",
    "def nrmse(y_true, y_pred):\n",
    "    if len(y_true) != len(y_pred):\n",
    "        return 0\n",
    "    # y_bar = np.array([np.mean(y_true) for _ in range(len(y_true))])\n",
    "    # return sigmoid2(rmse(y_true, y_pred) / rmse(y_true, y_bar))\n",
    "    return sigmoid2(1 - r2_score(y_true, y_pred))\n",
    "\n",
    "\n",
    "def nlev(y_true, y_pred):\n",
    "    if len(y_true) != len(y_pred):\n",
    "        return 0\n",
    "    return sigmoid2(sum([lev.distance(y_t, y_p) for y_t, y_p in zip(y_true, y_pred)]) / sum([len(y) for y in y_true]))\n",
    "\n",
    "\n",
    "def calculate_score(pred, gt):\n",
    "    if pred[\"class\"] != gt[\"class\"]:\n",
    "        return 0\n",
    "\n",
    "    if len(pred[\"value\"]) != len(gt[\"value\"]):\n",
    "        return 0\n",
    "    \n",
    "    if len(pred[\"value\"]) == 0 and len(gt[\"value\"]) == 0:\n",
    "        return 1\n",
    "\n",
    "    pred_xs = [x[\"x\"] for x in pred[\"value\"]]\n",
    "    pred_ys = [x[\"y\"] for x in pred[\"value\"]]\n",
    "\n",
    "    gt_xs = [x[\"x\"] for x in gt[\"value\"]]\n",
    "    gt_ys = [x[\"y\"] for x in gt[\"value\"]]\n",
    "\n",
    "    score = 0\n",
    "    if isinstance(gt_xs[0], str):\n",
    "        score += nlev(pred_xs, gt_xs)\n",
    "    else:\n",
    "        score += nrmse(pred_xs, gt_xs)\n",
    "\n",
    "    if isinstance(gt_ys[0], str):\n",
    "        score += nlev(pred_ys, gt_ys)\n",
    "    else:\n",
    "        score += nrmse(pred_ys, gt_ys)\n",
    "\n",
    "    return score / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_score(\n",
    "    {\n",
    "        'class': 'scatter',\n",
    "        'value': [\n",
    "            {'x': 1949.4201, 'y': 66.683},\n",
    "            {'x': 1954.6107, 'y': 66.2785},\n",
    "            {'x': 1959.9936, 'y': 65.6718},\n",
    "            {'x': 1964.7997, 'y': 64.0537},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'class': 'scatter',\n",
    "        'value': [\n",
    "            {'x': 1949.4201, 'y': 6.683},\n",
    "            {'x': 1954.6107, 'y': 66.2785},\n",
    "            {'x': 1959.9936, 'y': 65.6718},\n",
    "            {'x': 1964.7997, 'y': 64.0537},\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare YoloX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of x, y pairs to coco format\n",
    "def convert_keypoints_data_to_coco_format(gts, folder=\"train\"):\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    classes = [\"value\", \"x\", \"y\", \"x_label\", \"y_label\"]\n",
    "\n",
    "    for i, (k, v) in tqdm(enumerate(gts.items())):\n",
    "        path = os.path.join(f\"./{folder}/images/\", k)\n",
    "        img = cv2.imread(path)\n",
    "        images.append({\n",
    "            \"id\": i,\n",
    "            \"file_name\": k,\n",
    "            \"width\": img.shape[1],\n",
    "            \"height\": img.shape[0],\n",
    "        })\n",
    "\n",
    "        for c in classes:\n",
    "            if c == \"value\":\n",
    "                for j, point in enumerate(v[c]):\n",
    "                    bbox = [point[\"x_pixel\"] - 5, point[\"y_pixel\"] - 5, 10, 10]\n",
    "                    annotations.append({\n",
    "                        \"id\": len(annotations),\n",
    "                        \"image_id\": i,\n",
    "                        \"category_id\": 1,\n",
    "                        \"bbox\": bbox,\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"area\": 100,\n",
    "                    })\n",
    "            elif c in [\"x\", \"y\"]:\n",
    "                for j, point in enumerate(v[c]):\n",
    "                    bbox = [point[\"tick_pt\"][\"x\"] - 5, point[\"tick_pt\"][\"y\"] - 5, 10, 10]\n",
    "                    annotations.append({\n",
    "                        \"id\": len(annotations),\n",
    "                        \"image_id\": i,\n",
    "                        \"category_id\": 2 if c == \"x\" else 3,\n",
    "                        \"bbox\": bbox,\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"area\": 100,\n",
    "                    })\n",
    "            # else:\n",
    "            #     for j, point in enumerate(v[c]):\n",
    "            #         bbox = [point[0], point[1], point[2] - point[0], point[3] - point[1]]\n",
    "            #         annotations.append({\n",
    "            #             \"id\": len(annotations),\n",
    "            #             \"image_id\": i,\n",
    "            #             \"category_id\": 4 if c == \"x_label\" else 5,\n",
    "            #             \"bbox\": bbox,\n",
    "            #             \"iscrowd\": 0,\n",
    "            #             \"area\": 100,\n",
    "            #         })\n",
    "\n",
    "    coco_annotations = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"value\",\n",
    "                \"supercategory\": \"value\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"x\",\n",
    "                \"supercategory\": \"x\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"name\": \"y\",\n",
    "                \"supercategory\": \"y\",\n",
    "            },\n",
    "            # {\n",
    "            #     \"id\": 4,\n",
    "            #     \"name\": \"x_label\",\n",
    "            #     \"supercategory\": \"x_label\",\n",
    "            # },\n",
    "            # {\n",
    "            #     \"id\": 5,\n",
    "            #     \"name\": \"y_label\",\n",
    "            #     \"supercategory\": \"y_label\",\n",
    "            # },\n",
    "        ],\n",
    "        \"type\": \"instances\",\n",
    "    }\n",
    "\n",
    "    return coco_annotations\n",
    "\n",
    "\n",
    "# convert to coco format\n",
    "train_coco_annotations = convert_keypoints_data_to_coco_format(train_keypoints_ground_truths)\n",
    "val_coco_annotations = convert_keypoints_data_to_coco_format(val_keypoints_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "with open(\"annotations/train_coco_annotations.json\", \"w\") as writer:\n",
    "    json.dump(train_coco_annotations, writer)\n",
    "\n",
    "with open(\"annotations/val_coco_annotations.json\", \"w\") as writer:\n",
    "    json.dump(val_coco_annotations, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize one image with coco annotations to check if everything is correct\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "def visualize_coco_annotations(coco_annotations, image_id):\n",
    "    coco = COCO()\n",
    "    coco.dataset = coco_annotations\n",
    "    coco.createIndex()\n",
    "\n",
    "    img = coco.loadImgs(image_id)[0]\n",
    "    I = cv2.imread(os.path.join(\"./validation/images/\", img[\"file_name\"]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(I)\n",
    "\n",
    "    annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "    anns = coco.loadAnns(annIds)\n",
    "\n",
    "    for ann in anns:\n",
    "        bbox = ann[\"bbox\"]\n",
    "        # different colors for different classes\n",
    "        if ann[\"category_id\"] == 1:\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "        elif ann[\"category_id\"] == 2:\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor=\"g\", facecolor=\"none\")\n",
    "        elif ann[\"category_id\"] == 3:\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor=\"b\", facecolor=\"none\")\n",
    "        elif ann[\"category_id\"] == 4:\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor=\"y\", facecolor=\"none\")\n",
    "        elif ann[\"category_id\"] == 5:\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor=\"c\", facecolor=\"none\")\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    # increase size of image\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()\n",
    "\n",
    "index = random.randint(0, len(val_coco_annotations[\"images\"]))\n",
    "visualize_coco_annotations(val_coco_annotations, index)\n",
    "\n",
    "print(val_coco_annotations[\"images\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wrong data: 374fcc8c9542.jpg, a80688cb2101.jpg, b9507ab4e9fc.jpg, 3d0b1f8e6f61.jpg, 907a4cbc9ff1.jpg, fbf4820dc2c1.jpg\n",
    "\n",
    "# image_name = \"374fcc8c9542.jpg\"\n",
    "# points = train_keypoints_ground_truths[image_name][\"value\"]\n",
    "\n",
    "# # visualize \n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# def visualize_points(img, points):\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.imshow(img)\n",
    "\n",
    "#     for point in points:\n",
    "#         rect = patches.Rectangle((point[\"x_pixel\"] - 5, point[\"y_pixel\"] - 5), 10, 10, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "#         plt.gca().add_patch(rect)\n",
    "    \n",
    "#     # increase size of image\n",
    "#     fig = plt.gcf()\n",
    "#     fig.set_size_inches(18.5, 10.5)\n",
    "#     plt.show()\n",
    "\n",
    "# img = cv2.imread(os.path.join(\"./train/images/\", image_name))\n",
    "# visualize_points(img, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !cd external_data/chartinfo && \\\n",
    "# #     unzip -qq ICDAR2023_CHARTINFO_IIITH_UB_UNITEC_PMC_TEST_TASK3_v1.0.zip && \\\n",
    "# #     unzip -qq ICDAR2023_CHARTINFO_IIITH_UB_UNITEC_PMC_TEST_v1.0___NO_TASK3.zip && \\\n",
    "# #     unzip -qq ICDAR2023_CHARTINFO_UB_UNITEC_PMC_TRAIN_V1.0.zip\n",
    "\n",
    "# !cd external_data/chartqa && unzip -qq ChartQA\\ Dataset.zip\n",
    "\n",
    "# !cd external_data/dvqa && tar -xzf images.tar.gz && tar -xzf metadata.tar.gz\n",
    "\n",
    "# !cd external_data/figureqa && \\\n",
    "#     tar -xzf figureqa-sample-train-v1.tar.gz && \\\n",
    "#     tar -xzf figureqa-test1-v1.tar.gz && \\\n",
    "#     tar -xzf figureqa-test2-v1.tar.gz && \\\n",
    "#     tar -xzf figureqa-train1-v1.tar.gz && \\\n",
    "#     tar -xzf figureqa-validation1-v1.tar.gz && \\\n",
    "#     tar -xzf figureqa-validation2-v1.tar.gz\n",
    "\n",
    "# !cd external_data/plotqa/Images/Train && tar -xzf png.tar.gz\n",
    "# !cd external_data/plotqa/Images/Test && tar -xzf png.tar.gz\n",
    "# !cd external_data/plotqa/Images/Validation && tar -xzf png.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should convert the data to the competition format then use the old code to generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chartinfo\n",
    "root_annotation_folder = \"./external_data/chartinfo/annotations_JSON/\"\n",
    "root_image_folder = \"./external_data/chartinfo/images/\"\n",
    "extracted_annotation_folder = \"./external_data/chartinfo/extracted_annotations/\"\n",
    "\n",
    "if not os.path.exists(extracted_annotation_folder):\n",
    "    os.makedirs(extracted_annotation_folder)\n",
    "\n",
    "\n",
    "usable_graph_types = [\"horizontal_bar\", \"line\", \"scatter\", \"scatter-line\", \"vertical_bar\"]\n",
    "mapping = {\n",
    "    \"horizontal_bar\": \"horizontal_bar\",\n",
    "    \"line\": \"line\",\n",
    "    \"scatter\": \"scatter\",\n",
    "    \"scatter-line\": \"scatter\",\n",
    "    \"vertical_bar\": \"vertical_bar\"\n",
    "}\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "extracted_image_paths = []\n",
    "\n",
    "for graph_type in usable_graph_types:\n",
    "    annotation_folder = os.path.join(root_annotation_folder, graph_type)\n",
    "    image_folder = os.path.join(root_image_folder, graph_type)\n",
    "\n",
    "    for annotation_file in tqdm(os.listdir(annotation_folder)):\n",
    "        total += 1\n",
    "        data = json.load(open(os.path.join(annotation_folder, annotation_file)))\n",
    "        # task1 output is graph type\n",
    "        # task2 output is text location\n",
    "        # task3 output is type of text\n",
    "        # task4 output is x/y axis points\n",
    "        # task5 is legends analysis\n",
    "        # task6 output is final outputs\n",
    "        if data.get(\"task4\") is not None and data.get(\"task6\") is not None and len(data[\"task6\"][\"output\"][\"data series\"]) == 1:\n",
    "            image_path = os.path.join(image_folder, annotation_file.split(\".\")[0] + \".jpg\")\n",
    "            count += 1\n",
    "\n",
    "            # text data\n",
    "            id_to_role = {\n",
    "                item[\"id\"]: item[\"role\"] for item in data[\"task3\"][\"output\"][\"text_roles\"]\n",
    "            }\n",
    "            text_data = []\n",
    "            for item in data[\"task2\"][\"output\"][\"text_blocks\"]:\n",
    "                item[\"role\"] = id_to_role[item[\"id\"]]\n",
    "                text_data.append(item)\n",
    "\n",
    "            # axes data\n",
    "            axes_data = {\n",
    "                \"x-axis\": {\n",
    "                    \"ticks\": data[\"task4\"][\"output\"][\"axes\"][\"x-axis\"]\n",
    "                },\n",
    "                \"y-axis\": {\n",
    "                    \"ticks\": data[\"task4\"][\"output\"][\"axes\"][\"y-axis\"]\n",
    "                }\n",
    "            }\n",
    "            if graph_type in [\"scatter\", \"scatter-line\"]:\n",
    "                axes_data[\"x-axis\"][\"values-type\"] = \"numerical\"\n",
    "                axes_data[\"y-axis\"][\"values-type\"] = \"numerical\"\n",
    "            elif graph_type in [\"vertical_bar\", \"line\"]:\n",
    "                axes_data[\"x-axis\"][\"values-type\"] = \"categorical\"\n",
    "                axes_data[\"y-axis\"][\"values-type\"] = \"numerical\"\n",
    "            elif graph_type in [\"horizontal_bar\"]:\n",
    "                axes_data[\"x-axis\"][\"values-type\"] = \"numerical\"\n",
    "                axes_data[\"y-axis\"][\"values-type\"] = \"categorical\"\n",
    "\n",
    "            extracted_annotation = {\n",
    "                \"source\": \"external\",\n",
    "                \"chart-type\": mapping[graph_type],\n",
    "                \"text\": text_data,\n",
    "                \"axes\": axes_data,\n",
    "                \"data-series\": data[\"task6\"][\"output\"][\"data series\"][0][\"data\"],\n",
    "            }\n",
    "            \n",
    "            # save annotation\n",
    "            with open(os.path.join(extracted_annotation_folder, annotation_file), \"w\") as f:\n",
    "                json.dump(extracted_annotation, f, indent=4)\n",
    "\n",
    "            extracted_image_paths.append(image_path)\n",
    "    \n",
    "print(f\"Extracted {count} images out of {total} images\")\n",
    "# write extracted_image_paths to a file\n",
    "with open(\"./external_data/chartinfo/extracted_image_paths.txt\", \"w\") as f:\n",
    "    for path in extracted_image_paths:\n",
    "        f.write(path + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chartqa\n",
    "root_annotation_folder = \"./external_data/chartqa/ChartQA Dataset/train/annotations\"\n",
    "root_image_folder = \"./external_data/chartqa/ChartQA Dataset/train/png\"\n",
    "extracted_annotation_folder = \"./external_data/chartqa/ChartQA Dataset/extracted_annotations/\"\n",
    "\n",
    "if not os.path.exists(extracted_annotation_folder):\n",
    "    os.makedirs(extracted_annotation_folder)\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "\n",
    "for annotation_file in tqdm(os.listdir(root_annotation_folder)):\n",
    "    with open(os.path.join(root_annotation_folder, annotation_file)) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    break\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X/Y labels detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text recognition - parseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
